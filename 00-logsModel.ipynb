{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6381bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import relevant modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# The following lines adjust the granularity of reporting. \n",
    "pd.options.display.max_rows = 10\n",
    "pd.options.display.float_format = \"{:.1f}\".format\n",
    "\n",
    "# The following line improves formatting when ouputting NumPy arrays.\n",
    "np.set_printoptions(linewidth = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "919a4c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset\n",
    "dataset = pd.read_csv('log2.csv')\n",
    "dataset['Action'] = np.where(dataset['Action'] == 'allow', 1, 0) #encoding dependant output\n",
    "X = dataset.iloc[:, :-1].values\n",
    "y = dataset.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1bb025c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "45581951",
   "metadata": {},
   "source": [
    "# Encoding data\n",
    "    Action the dependant variable contain 3 targets\n",
    "* allow -> [100]\n",
    "* deny  -> [010]\n",
    "* drop  -> [001]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0603782f",
   "metadata": {},
   "source": [
    "#Encoding data\n",
    "dummies = pd.get_dummies(dataset.Action)  \n",
    "merged = pd.concat([dataset,dummies],axis=1)\n",
    "final = merged.drop(['Action'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d733ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0c4aa8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, ..., 1, 0, 0])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64d811d",
   "metadata": {},
   "source": [
    "## Performing Feature Scalling\n",
    "* As you can see in the dataset, all values are not in the same range,  and that requires a lot of time for calculation. So to    overcome this problem, we perform feature scaling.\n",
    "\n",
    "\n",
    "* Feature scaling help us to normalize the data within a particular range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81bfbe16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train_normalized = sc.fit_transform(X_train)\n",
    "X_test_normalized = sc.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4be4b5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(65532, 11)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c79490da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the plot_curve function.\n"
     ]
    }
   ],
   "source": [
    "def plot_curve(epochs, hist, list_of_metrics):\n",
    "    plt.figure()\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Value\")\n",
    "    \n",
    "    for m in list_of_metrics:\n",
    "        x = hist[m]\n",
    "        plt.plot(epochs[1:], x[1:], label=m)\n",
    "    \n",
    "    plt.legend()\n",
    "\n",
    "print(\"Loaded the plot_curve function.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8181a465",
   "metadata": {},
   "source": [
    "## Create a Deep Neural Net Model\n",
    "\n",
    "* input 11 unites, for 11 features we have in Dataset\n",
    "* hidden 150 unites estimated for better performance\n",
    "* output 3 unites we have [100], [010], [001] three labes index of 1 will define the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50a26ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(my_learning_rate):\n",
    "    model = tf.keras.models.Sequential()\n",
    "  \n",
    "  # Define the input layer and first hidden layer.\n",
    "    model.add(Dense(units=11, kernel_initializer='normal', activation='relu'))\n",
    "    \n",
    "# Define the input layer and first hidden layer.\n",
    "    model.add(Dense(units=150, activation='relu'))\n",
    "  # Define a dropout regularization layer.\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.Dropout(rate=0.2))\n",
    "\n",
    "  # Define the output layer. \n",
    "    model.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))     \n",
    "                           \n",
    "  # Construct the layers into a model that TensorFlow can execute.\n",
    "    model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82472747",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_features, train_label, epochs,\n",
    "                batch_size, validation_split):\n",
    "    \n",
    "    history = model.fit(train_features, train_label, batch_size,\n",
    "                      epochs=epochs, \n",
    "                      validation_split=validation_split)\n",
    " \n",
    "  # To track the progression of training, gather a snapshot\n",
    "  # of the model's metrics at each epoch. \n",
    "    epochs = history.epoch\n",
    "    hist = pd.DataFrame(history.history)\n",
    "    return epochs, hist "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d2fb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameters\n",
    "learning_rate = 0.003 #\n",
    "epochs = 10\n",
    "batch_size = 30 #\n",
    "validation_split = 0.2 #\n",
    "\n",
    "# Establish the model's topography.\n",
    "my_model = create_model(learning_rate)\n",
    "\n",
    "# Train the model on the normalized training set.\n",
    "epochs, hist = train_model(my_model, X_train_normalized, y_train, \n",
    "                           epochs, batch_size, validation_split)\n",
    "# Plot a graph of the metric vs. epochs.\n",
    "list_of_metrics_to_plot = ['accuracy']\n",
    "plot_curve(epochs, hist, list_of_metrics_to_plot)\n",
    "\n",
    "# Evaluate against the test set.\n",
    "print(\"\\n Evaluate the new model against the test set:\")\n",
    "my_model.evaluate(x=X_test_normalized, y=y_test, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2530f7c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 11)                132       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 150)               1800      \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 150)              600       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 150)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 151       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,683\n",
      "Trainable params: 2,383\n",
      "Non-trainable params: 300\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "my_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "593874cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model.save('logsModel.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
